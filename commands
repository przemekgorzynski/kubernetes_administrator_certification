kubectl run nginx --image nginx      #rum simple nginx pod
kubectl get nodes
kubectl cluster-info
kubectl get all

#namespace
By default we are in DEFAULT namespace
Kubectl get pods --namespace=NAME
Kubectl create -f pod-definition.yml --namespace=dev  # see pods in dev namespace from default namespace point of view
kubectl  create namespace NAME
kubectl config set-context $(kubectl config current-context) --namespace=dev   # pernamently switching to dev namespace
kubectl get pods --all-namespaces

#pod
kubectl get pods
kubectl describe pod NAME
kubectl delete pod NAME

#replication controller
kubectl create -f replicationcontroller.yml
kubectl get replicationcontroller

#replicaset
kubectl edit replicaset NAME  - edit existing replicaset when don't know whem definition file is
kubectl create -f replicaset-definition.yaml
kubectl get replicaset
kubectl describe replicaset
kubectl delete replicaset myapp-replicaset
kubectl replace -f replicaset-definition.yml    # BEST OPTION - update replicaset - eg. when number of replicas changed  (have to modify YML file)
kubectl scale -–replicas=6 –f replicaset-definition.yml  #update replicaset  (on the fly - change definition file)
kubectl scale -–replicas=6 replicaset REPLICA_NAME   #update replicaset  (on the fly without modyfying definition file)


#deployment
kubectl create -f deployment-definition.yaml
kubectl get deployment
kubectl describe deployment
kubectl apply –f deployment-definition.yml   #run deployments after changing definition file (eg new version)
kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1  # set new image to deployment
kubectl rollout status deployment/myapp-deployment    #status of deployment (rollout)
kubectl rollout history deployment/myapp-deployment   #history of rollouts
kubectl rollout undo deployment/myapp-deployment      #falling back rollout

#services  (types NodePort, ClusterIP, LoadBalancer)
kubectl create -f service-definition.yaml
kubectl get services
kubectl describe service
types:
 - ClusterIP  - internal service between pods (all to all)
 - LoadBalancer - can be reach from outside (on clouds provider)
 - NodePort - From one service to multiple pods within the same service

 IMPERATIVE COMMANDS
====================================================================================================
#create pod from cli without YAML file
kubectl run --generator=run-pod/v1 nginx --image=nginx  -l tier=db --dry-run -o yaml  # with label tier: db

# DRY RUN DOES NOT CREATE OBJECT, JUST TEST
# - o yaml prints output how yaml file should be written
# master $ kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
master $
==========================================================================================================
#create service from cli without YAML file
# expose by default adds labels from pod

kubectl expose pod redis --port=6379 --name redis-service -o yaml --dry-run
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    tier: db
  name: redis-service
spec:
  ports:
  - port: 6379
    protocol: TCP
    targetPort: 6379
  selector:
    tier: db
status:
  loadBalancer: {}

==============================================================================================================
#create deployment from cli without YAML file
# this way we cannot use replicas. Need to run scale command safter ddepoyment definitiov
# kubectl scale deployment/webapp --replicas=3

master $ kubectl create deployment webapp --image=kodekloud/webapp-color --dry-run -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: webapp
  name: webapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webapp
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color
        name: webapp-color
        resources: {}
status: {}

==============================SCHEDULING==================================

kubectl get pods --selector app=App1  # get pods with label app=App1
kubectl get all --selector env=prod,bu=finance,tier=frontend

kubectl taint nodes node01 spray=mortein:NoSchedule
kubectl describe node kubemaster | grep taint

kubectl taint nodes master node-role.kubernetes.io/master:NoSchedule-             # removing taints

kubectl label nodes NAME size=large           # label nodes with size=large label


=========================== Logging / monitoring =============================================================

No official logging tool - can deploy metric server

git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git

kubectl logs -f POD_NAME
kubectl logs -f POD_NAME CONTAINER_NAME   in case pod consist of multiple containers

============================ APP LIFECYCLE ====================================================================
kubectl apply –f deployment-definition.yml   #run deployments after changing definition file (eg new version)
kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1  # set new image to deployment
kubectl rollout status deployment/myapp-deployment    #status of deployment (rollout)
kubectl rollout history deployment/myapp-deployment   #history of rollouts
kubectl rollout undo deployment/myapp-deployment      #falling back rollout

======================= ENVS / SECRETS===========================================================================
# config map is a input with variables. When we want to specify source of variables to be used in many pods instead of
# configuring it inside each pod. Setting config map with command line

# FILE AND COMMAND SARE THE SAME FOR BOTH SECRETS AND ENV

kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123

kubecrl create -f config-map.yml

================ MAINTANANCE ===============================================================================

kubectl drain NODE_NAME   # move pods from node to another (make node redy to reboot, upgrade ), node is marked as unaschedulable
kubectl cordon NODE_NAME # mark pod as unschedulable, but do not remove / move pods from node
kubectl uncordon NODE_NAME  # make pod available again for scheduling after coming back, pods don't automatically go back to this pod

# if we have pod not managed by by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet it will remove it while drain

kubeadm upgrade plan


#upgradin plan

#MASTER
apt-get upgrade -y kubeadm=1.12.0-00
kubeadm upgrade apply v1.12.0
apt-get upgrade -y kubelet=1.12.0-00
systemctl restart kubelet

#NODES
kubectl drain NODE
kubectl cordon NODE
apt-get upgrade -y kubeadm=1.12.0-00
apt-get upgrade -y kubelet=1.12.0-00
kubeadm upgrade node config --kubelet-version v1.12.0
systemctl restart kubelet
kubectl uncordon NODE



#BACKUP

kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml      # create a copy of all objects in cluster

export ETCDCTL_API=3
etcdctl  snapshot save snapshot.db


====================================ROLES======================================
Firs tneed to creat role and role binding with yaml file

kubectl get roles
kubectl get rolebindings
kubectl describe role NAME

kubectl auth ca-i delete nodes   # check if we can perform operations of deleting nodes
kubectl auth can-i create pods --as dev-user  # checking permissions as another user